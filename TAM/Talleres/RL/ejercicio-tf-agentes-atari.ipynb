{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":1229.318887,"end_time":"2025-02-19T20:30:04.660782","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-02-19T20:09:35.341895","version":"2.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Reinforcement Learning / Tensorflow - TF_Agents \n\nEl aprendizaje por refuerzo (RL) es uno de los campos más antiguos del aprendizaje automático. Ha existido desde la década de 1950 y ha producido muchas aplicaciones interesantes a lo largo de los años.\n\n<br />\n<img src='https://es.mathworks.com/help///reinforcement-learning/ug/reinforcement_learning_diagram.png' width='300' />\n\n*\"El aprendizaje por refuerzo se diferencia del aprendizaje supervisado en que no requiere la presentación de pares de entrada/salida etiquetados y no requiere que se corrijan explícitamente acciones subóptimas. En cambio, la atención se centra en encontrar un equilibrio entre la exploración (de territorio desconocido) y la explotación (del conocimiento actual)..\"* [wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning)","metadata":{"papermill":{"duration":0.006848,"end_time":"2025-02-19T20:09:38.363074","exception":false,"start_time":"2025-02-19T20:09:38.356226","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#librerias necesarias\n!sudo apt-get update\n!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n!pip install 'imageio==2.4.0'\n!pip install pyvirtualdisplay\n!pip install tf-agents[reverb]\n!pip install pyglet\n!pip install swig\n!pip install gym[atari,box2d,accept-rom-license]  #install gym and virtual display","metadata":{"id":"KEHR2Ui-lo8O","papermill":{"duration":209.015072,"end_time":"2025-02-19T20:13:07.38468","exception":false,"start_time":"2025-02-19T20:09:38.369608","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:14:25.417892Z","iopub.execute_input":"2025-02-25T15:14:25.41816Z","iopub.status.idle":"2025-02-25T15:17:20.807125Z","shell.execute_reply.started":"2025-02-25T15:14:25.418132Z","shell.execute_reply":"2025-02-25T15:17:20.806089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from __future__ import absolute_import, division, print_function\n\nimport base64\nimport imageio\nimport IPython\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL.Image\nimport pyvirtualdisplay\nimport reverb\n\nimport tensorflow as tf\n\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.drivers import py_driver\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.networks import sequential\nfrom tf_agents.policies import py_tf_eager_policy\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import reverb_replay_buffer\nfrom tf_agents.replay_buffers import reverb_utils\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import common\n\nfrom tf_agents.environments import  suite_gym\nfrom tf_agents.environments.atari_preprocessing import AtariPreprocessing\nfrom tf_agents.environments.atari_wrappers import FrameStack4\nimport gym\n\n# To get smooth animations\nimport matplotlib.animation as animation\nmatplotlib.rc('animation', html='jshtml')","metadata":{"id":"sMitx5qSgJk1","papermill":{"duration":6.408044,"end_time":"2025-02-19T20:13:13.81839","exception":false,"start_time":"2025-02-19T20:13:07.410346","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:17:20.808188Z","iopub.execute_input":"2025-02-25T15:17:20.808491Z","iopub.status.idle":"2025-02-25T15:17:27.561633Z","shell.execute_reply.started":"2025-02-25T15:17:20.80846Z","shell.execute_reply":"2025-02-25T15:17:27.560744Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setup...\n**Aprendendiendo a jugar River Raid** <br />\n<img src='https://s2.glbimg.com/bQhuS5w10e3MAFxzNtnBm_jJNVA=/695x0/s.glbimg.com/po/tt2/f/original/2016/02/26/river-raid-atari-2600-8.jpg' width='300' />\n<br />\n*\"River Raid: Varias revistas lo votaron como el mejor juego del año. En 1983, InfoWorld lo llamó el “videojuego” más desafiante.[2] En 1984, la revista The Desert News lo llamó: “El juego de guerra más jugable y divertido”.[3] Ese mismo año, el juego recibió el premio al \"mejor juego de acción del año\"1984\"[4], y un certificado al mérito en la categoría.\"1984 Best Computer Action Game\"* [Wikipedia](https://pt.wikipedia.org/wiki/River_Raid)","metadata":{"papermill":{"duration":0.026567,"end_time":"2025-02-19T20:13:13.872519","exception":false,"start_time":"2025-02-19T20:13:13.845952","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#Carregando - River raid\nenv = suite_gym.load(environment_name=\"ALE/Pong-v5\",max_episode_steps=20, gym_env_wrappers=[AtariPreprocessing,FrameStack4])\nenv","metadata":{"papermill":{"duration":0.21714,"end_time":"2025-02-19T20:13:14.115061","exception":false,"start_time":"2025-02-19T20:13:13.897921","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:17:27.562504Z","iopub.execute_input":"2025-02-25T15:17:27.563091Z","iopub.status.idle":"2025-02-25T15:17:27.745079Z","shell.execute_reply.started":"2025-02-25T15:17:27.563065Z","shell.execute_reply":"2025-02-25T15:17:27.744403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"env.gym","metadata":{"papermill":{"duration":0.034276,"end_time":"2025-02-19T20:13:14.176549","exception":false,"start_time":"2025-02-19T20:13:14.142273","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:17:27.746997Z","iopub.execute_input":"2025-02-25T15:17:27.747205Z","iopub.status.idle":"2025-02-25T15:17:27.751472Z","shell.execute_reply.started":"2025-02-25T15:17:27.747188Z","shell.execute_reply":"2025-02-25T15:17:27.750784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gym.envs.registry","metadata":{"papermill":{"duration":0.03571,"end_time":"2025-02-19T20:13:14.237361","exception":false,"start_time":"2025-02-19T20:13:14.201651","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:17:27.752908Z","iopub.execute_input":"2025-02-25T15:17:27.753123Z","iopub.status.idle":"2025-02-25T15:17:27.768511Z","shell.execute_reply.started":"2025-02-25T15:17:27.753105Z","shell.execute_reply":"2025-02-25T15:17:27.767955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"env.seed(42)\nenv.reset()","metadata":{"papermill":{"duration":0.226335,"end_time":"2025-02-19T20:13:14.490923","exception":false,"start_time":"2025-02-19T20:13:14.264588","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:17:27.769455Z","iopub.execute_input":"2025-02-25T15:17:27.76964Z","iopub.status.idle":"2025-02-25T15:17:27.970789Z","shell.execute_reply.started":"2025-02-25T15:17:27.769624Z","shell.execute_reply":"2025-02-25T15:17:27.970142Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"env.reset()\nimg = env.render(mode=\"rgb_array\")\nplt.figure(figsize=(4, 6))\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()","metadata":{"papermill":{"duration":0.140357,"end_time":"2025-02-19T20:13:14.659646","exception":false,"start_time":"2025-02-19T20:13:14.519289","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:17:27.971623Z","iopub.execute_input":"2025-02-25T15:17:27.971937Z","iopub.status.idle":"2025-02-25T15:17:28.095114Z","shell.execute_reply.started":"2025-02-25T15:17:27.971908Z","shell.execute_reply":"2025-02-25T15:17:28.094419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up a virtual display for rendering OpenAI gym environments.\ndisplay = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()","metadata":{"id":"J6HsdS5GbSjd","papermill":{"duration":0.525478,"end_time":"2025-02-19T20:13:15.265275","exception":false,"start_time":"2025-02-19T20:13:14.739797","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:17:28.095786Z","iopub.execute_input":"2025-02-25T15:17:28.096048Z","iopub.status.idle":"2025-02-25T15:17:28.54457Z","shell.execute_reply.started":"2025-02-25T15:17:28.096029Z","shell.execute_reply":"2025-02-25T15:17:28.543934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.version.VERSION","metadata":{"id":"NspmzG4nP3b9","papermill":{"duration":0.035094,"end_time":"2025-02-19T20:13:15.327718","exception":false,"start_time":"2025-02-19T20:13:15.292624","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:17:28.545352Z","iopub.execute_input":"2025-02-25T15:17:28.545633Z","iopub.status.idle":"2025-02-25T15:17:28.550694Z","shell.execute_reply.started":"2025-02-25T15:17:28.545604Z","shell.execute_reply":"2025-02-25T15:17:28.549927Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Environment Specifications\nTF-Agents proporciona especificaciones para observaciones, acciones y pasos, incluidas sus respectivas formas.","metadata":{"papermill":{"duration":0.026797,"end_time":"2025-02-19T20:13:15.380783","exception":false,"start_time":"2025-02-19T20:13:15.353986","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print('Acciones disponibles:\\n{}\\r\\n'.format(env.gym.get_action_meanings()))\nprint('Observaciones:\\n{}'.format(env.observation_spec()))","metadata":{"papermill":{"duration":0.034998,"end_time":"2025-02-19T20:13:15.443547","exception":false,"start_time":"2025-02-19T20:13:15.408549","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:17:28.551511Z","iopub.execute_input":"2025-02-25T15:17:28.551778Z","iopub.status.idle":"2025-02-25T15:17:28.56609Z","shell.execute_reply.started":"2025-02-25T15:17:28.551748Z","shell.execute_reply":"2025-02-25T15:17:28.565254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Environment Wrappers","metadata":{"papermill":{"duration":0.028439,"end_time":"2025-02-19T20:13:15.502293","exception":false,"start_time":"2025-02-19T20:13:15.473854","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#Aquí está la lista de wrappers disponibles:\nimport tf_agents.environments.wrappers\n\nfor name in dir(tf_agents.environments.wrappers):\n    obj = getattr(tf_agents.environments.wrappers, name)\n    if hasattr(obj, \"__base__\") and issubclass(obj, tf_agents.environments.wrappers.PyEnvironmentBaseWrapper):\n        print(\"{:27s} {}\".format(name, obj.__doc__.split(\"\\n\")[0]))","metadata":{"papermill":{"duration":0.036181,"end_time":"2025-02-19T20:13:15.566363","exception":false,"start_time":"2025-02-19T20:13:15.530182","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:17:28.566942Z","iopub.execute_input":"2025-02-25T15:17:28.567132Z","iopub.status.idle":"2025-02-25T15:17:28.584236Z","shell.execute_reply.started":"2025-02-25T15:17:28.567112Z","shell.execute_reply":"2025-02-25T15:17:28.583665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Algunos ejemplos de acciones en el juego\nenv.reset()\ntime_step = env.step(np.array(1)) # FIRE\ntime_step = env.step(np.array(3)) # RIGHT\ntime_step = env.step(np.array(5)) # DOWNRIGHT\n\nobservation = time_step.observation.astype(np.float32)\n\n#Como existen 3 canales de colores, no podemos mostrar 4 frames.\nimage = observation[..., :3]\nimage = np.clip(image / 150, 0, 1)\nplt.imshow(image)\nplt.axis(\"off\")\nprint(observation.shape)","metadata":{"papermill":{"duration":0.108587,"end_time":"2025-02-19T20:13:15.702515","exception":false,"start_time":"2025-02-19T20:13:15.593928","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:19:04.739592Z","iopub.execute_input":"2025-02-25T15:19:04.739916Z","iopub.status.idle":"2025-02-25T15:19:04.815075Z","shell.execute_reply.started":"2025-02-25T15:19:04.739889Z","shell.execute_reply":"2025-02-25T15:19:04.814361Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Para agrupar el entorno utilizamos TFPyEnviroment.","metadata":{"papermill":{"duration":0.027626,"end_time":"2025-02-19T20:13:15.758865","exception":false,"start_time":"2025-02-19T20:13:15.731239","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tf_agents.environments.tf_py_environment import TFPyEnvironment\ntf_env = TFPyEnvironment(env)","metadata":{"papermill":{"duration":0.04166,"end_time":"2025-02-19T20:13:15.82826","exception":false,"start_time":"2025-02-19T20:13:15.7866","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:19:10.930114Z","iopub.execute_input":"2025-02-25T15:19:10.93039Z","iopub.status.idle":"2025-02-25T15:19:10.942904Z","shell.execute_reply.started":"2025-02-25T15:19:10.930369Z","shell.execute_reply":"2025-02-25T15:19:10.942058Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DQN\n- TF-Agents proporciona algunos paquetes de red.\n\n- En este paquete, las imágenes se almacenan utilizando bytes del 0 al 255 para utilizar menos RAM.\n","metadata":{"papermill":{"duration":0.027218,"end_time":"2025-02-19T20:13:15.883207","exception":false,"start_time":"2025-02-19T20:13:15.855989","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tf_agents.networks.q_network import QNetwork\n#convertir observacinoes a float float 32, normalizando.. (valores  0.0 a 1.0) \npreprocessing_layer = tf.keras.layers.Lambda( lambda obs: tf.cast(obs, np.float32) / 255.)\n\n#arquitectura:\n#conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\nconv_layer_params=[(32, (8, 8), 4) , (64, (4, 4), 2)]#, (64, (3, 3), 1), (1024, (7, 7), 1)]\n#layer dense com 512 por uma cama de sair de 4 unidades\nfc_layer_params=(1024,)\n\nq_network = QNetwork(tf_env.observation_spec(), tf_env.action_spec()\n                     ,preprocessing_layers=preprocessing_layer\n                     ,conv_layer_params=conv_layer_params\n                     ,fc_layer_params=fc_layer_params)\nq_network.summary","metadata":{"papermill":{"duration":0.086161,"end_time":"2025-02-19T20:13:15.996456","exception":false,"start_time":"2025-02-19T20:13:15.910295","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:19:14.212091Z","iopub.execute_input":"2025-02-25T15:19:14.212393Z","iopub.status.idle":"2025-02-25T15:19:14.268982Z","shell.execute_reply.started":"2025-02-25T15:19:14.212368Z","shell.execute_reply":"2025-02-25T15:19:14.268273Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DQN Agent\n[DQN paper ](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)  ","metadata":{"papermill":{"duration":0.028289,"end_time":"2025-02-19T20:13:16.054573","exception":false,"start_time":"2025-02-19T20:13:16.026284","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tf_agents.agents.dqn.dqn_agent import DqnAgent\n\ntrain_step = tf.Variable(0)\nupdate_period = 4 \n#optimizer = keras.optimizers.Adam(lr=2.5e-4, rho=0.95, momentum=0.1,epsilon=0.00001, centered=True)\noptimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate=2.5e-4, decay=0.95, momentum=0.0,\n                                     epsilon=0.00001, centered=True)\n#optimizer = keras.optimizers.Adam(lr=0.00005,epsilon=0.00001)\nepsilon_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=0.8,\n    decay_steps=250000 // update_period, \n    end_learning_rate=0.01)\n\nagent = DqnAgent(tf_env.time_step_spec(),\n                 tf_env.action_spec(),\n                 q_network=q_network,\n                 optimizer=optimizer,\n                 target_update_period=2000, \n                 #La función de pérdida debe devolver un error por instancia, por lo que definimos reducción=\"none\" \n                 td_errors_loss_fn=tf.keras.losses.Huber(reduction=\"none\"),\n                 gamma=0.89, # discount factor\n                 train_step_counter=train_step,\n                 epsilon_greedy=lambda: epsilon_fn(train_step))\nagent.initialize()","metadata":{"papermill":{"duration":0.729563,"end_time":"2025-02-19T20:13:16.811484","exception":false,"start_time":"2025-02-19T20:13:16.081921","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:19:58.382102Z","iopub.execute_input":"2025-02-25T15:19:58.382378Z","iopub.status.idle":"2025-02-25T15:20:00.040617Z","shell.execute_reply.started":"2025-02-25T15:19:58.382358Z","shell.execute_reply":"2025-02-25T15:20:00.039948Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Se utiliza la [Huber loss](https://en.wikipedia.org/wiki/Huber_loss) como balance entre mse y mae","metadata":{"papermill":{"duration":0.032615,"end_time":"2025-02-19T20:13:16.872584","exception":false,"start_time":"2025-02-19T20:13:16.839969","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Replay Buffer and the Corresponding Observer\n\n- La biblioteca TF-Agents proporciona algunas implementaciones de búfer de reproducción en el paquete tf_agents.replay_buffers.\n\n**max_length:**  1000000","metadata":{"papermill":{"duration":0.029904,"end_time":"2025-02-19T20:13:16.93227","exception":false,"start_time":"2025-02-19T20:13:16.902366","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tf_agents.replay_buffers import tf_uniform_replay_buffer\n\n#data_spec datos que se guardarán en el búfer.\n#batch_size es el número de trayectorias que se deben agregar a cada paso.\n#max_length es la longitud máxima de reproducción. (Documento DQN2015: Cuidado con el acaparador de RAM)\n\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer( data_spec=agent.collect_data_spec,    batch_size=tf_env.batch_size, max_length=300000)#ojo para el entrenamiento\n\nreplay_buffer_observer = replay_buffer.add_batch","metadata":{"papermill":{"duration":2.520603,"end_time":"2025-02-19T20:13:19.480735","exception":false,"start_time":"2025-02-19T20:13:16.960132","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:20:08.496072Z","iopub.execute_input":"2025-02-25T15:20:08.496347Z","iopub.status.idle":"2025-02-25T15:20:11.312428Z","shell.execute_reply.started":"2025-02-25T15:20:08.496326Z","shell.execute_reply":"2025-02-25T15:20:11.311742Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Metrics\n\nUsando las diversas métricas del paquete  tf_agents.metrics.","metadata":{"papermill":{"duration":0.027663,"end_time":"2025-02-19T20:13:19.537471","exception":false,"start_time":"2025-02-19T20:13:19.509808","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tf_agents.metrics import tf_metrics\nfrom tf_agents.eval.metric_utils import log_metrics\nimport logging\n\ntrain_metrics = [\n    tf_metrics.NumberOfEpisodes(),\n    tf_metrics.EnvironmentSteps(),\n    tf_metrics.AverageReturnMetric(),\n    tf_metrics.AverageEpisodeLengthMetric(),\n]\nlogging.getLogger().setLevel(logging.INFO)\nlog_metrics(train_metrics)","metadata":{"papermill":{"duration":0.165736,"end_time":"2025-02-19T20:13:19.730617","exception":false,"start_time":"2025-02-19T20:13:19.564881","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:20:13.74461Z","iopub.execute_input":"2025-02-25T15:20:13.74496Z","iopub.status.idle":"2025-02-25T15:20:13.893062Z","shell.execute_reply.started":"2025-02-25T15:20:13.744928Z","shell.execute_reply":"2025-02-25T15:20:13.892169Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Collect Driver\n\nUn collect driver es un objeto que explora un entorno mediante políticas, recoge experiencias de cada etapa y las transmite a los observadores.","metadata":{"papermill":{"duration":0.026386,"end_time":"2025-02-19T20:13:19.78537","exception":false,"start_time":"2025-02-19T20:13:19.758984","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n\ncollect_driver = DynamicStepDriver(\n    tf_env,\n    agent.collect_policy,\n    observers=[replay_buffer_observer] + train_metrics,\n    num_steps=update_period) \ncollect_driver","metadata":{"papermill":{"duration":0.035508,"end_time":"2025-02-19T20:13:19.847128","exception":false,"start_time":"2025-02-19T20:13:19.81162","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:20:16.565064Z","iopub.execute_input":"2025-02-25T15:20:16.565339Z","iopub.status.idle":"2025-02-25T15:20:16.570933Z","shell.execute_reply.started":"2025-02-25T15:20:16.565317Z","shell.execute_reply":"2025-02-25T15:20:16.570104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tf_agents.policies.random_tf_policy import RandomTFPolicy\n\nclass ShowProgress:\n    def __init__(self, total):\n        self.counter = 0\n        self.total = total\n    def __call__(self, trajectory):\n        if not trajectory.is_boundary():\n            self.counter += 1\n        if self.counter % 100 == 0:\n            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")\n\ninitial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n                                        tf_env.action_spec())\ninit_driver = DynamicStepDriver(\n    tf_env,\n    initial_collect_policy,\n    observers=[replay_buffer.add_batch, ShowProgress(20000)],\n    num_steps=20000)\nfinal_time_step, final_policy_state = init_driver.run()","metadata":{"papermill":{"duration":112.426373,"end_time":"2025-02-19T20:15:12.301221","exception":false,"start_time":"2025-02-19T20:13:19.874848","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:20:19.585169Z","iopub.execute_input":"2025-02-25T15:20:19.585518Z","iopub.status.idle":"2025-02-25T15:20:20.929671Z","shell.execute_reply.started":"2025-02-25T15:20:19.585493Z","shell.execute_reply":"2025-02-25T15:20:20.928756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Ejemplo de la trayectoria final de un episodio\ntrajectories, buffer_info = replay_buffer.get_next(sample_batch_size=2, num_steps=17)\ntrajectories, buffer_info, trajectories._fields","metadata":{"papermill":{"duration":0.076793,"end_time":"2025-02-19T20:15:12.416625","exception":false,"start_time":"2025-02-19T20:15:12.339832","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:20:28.502093Z","iopub.execute_input":"2025-02-25T15:20:28.502403Z","iopub.status.idle":"2025-02-25T15:20:28.542334Z","shell.execute_reply.started":"2025-02-25T15:20:28.502377Z","shell.execute_reply":"2025-02-25T15:20:28.54161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tf_agents.trajectories.trajectory import to_transition\n\ntime_steps, action_steps, next_time_steps = to_transition(trajectories)\ntime_steps.observation.shape,trajectories.step_type.numpy()\n\n\nplt.figure(figsize=(10, 6.8))\nfor row in range(2):\n    for col in range(3):\n        plt.subplot(2, 3, row * 3 + col + 1)\n        obs = trajectories.observation[row, col].numpy().astype(np.float32)\n        img = obs[..., :3]\n        current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)\n        img[..., 0] += current_frame_delta\n        img[..., 2] += current_frame_delta\n        img = np.clip(img / 150, 0, 1)\n        plt.imshow(img)\n        plt.axis(\"off\")\nplt.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0, wspace=0.02)\nplt.show()","metadata":{"papermill":{"duration":0.313091,"end_time":"2025-02-19T20:15:12.76733","exception":false,"start_time":"2025-02-19T20:15:12.454239","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:20:43.901494Z","iopub.execute_input":"2025-02-25T15:20:43.90179Z","iopub.status.idle":"2025-02-25T15:20:44.192562Z","shell.execute_reply.started":"2025-02-25T15:20:43.901769Z","shell.execute_reply":"2025-02-25T15:20:44.19181Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset\n\nConvertir los datos del búfer en un conjunto de datos para el entrenamiento","metadata":{"papermill":{"duration":0.036103,"end_time":"2025-02-19T20:15:12.846822","exception":false,"start_time":"2025-02-19T20:15:12.810719","status":"completed"},"tags":[]}},{"cell_type":"code","source":"dataset = replay_buffer.as_dataset(\n    sample_batch_size=64,\n    num_steps=17,\n    num_parallel_calls=3\n).prefetch(3)\niterator = iter(dataset)\ntrajectories, buffer_info = next(iterator)\nplt.figure(figsize=(10, 6.8))\nfor row in range(2):\n    for col in range(3):\n        plt.subplot(2, 3, row * 3 + col + 1)\n        obs = trajectories.observation[row, col].numpy().astype(np.float32)\n        img = obs[..., :3]\n        current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)\n        img[..., 0] += current_frame_delta\n        img[..., 2] += current_frame_delta\n        img = np.clip(img / 150, 0, 1)\n        plt.imshow(img)\n        plt.axis(\"off\")\nplt.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0, wspace=0.02)\nplt.show()\n","metadata":{"papermill":{"duration":1.31572,"end_time":"2025-02-19T20:15:14.199483","exception":false,"start_time":"2025-02-19T20:15:12.883763","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:20:48.386035Z","iopub.execute_input":"2025-02-25T15:20:48.386317Z","iopub.status.idle":"2025-02-25T15:20:49.382862Z","shell.execute_reply.started":"2025-02-25T15:20:48.386296Z","shell.execute_reply":"2025-02-25T15:20:49.382061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = replay_buffer.as_dataset(\n    sample_batch_size=64,\n    num_steps=2,\n    num_parallel_calls=3).prefetch(3)","metadata":{"papermill":{"duration":0.134703,"end_time":"2025-02-19T20:15:14.376721","exception":false,"start_time":"2025-02-19T20:15:14.242018","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:20:52.952114Z","iopub.execute_input":"2025-02-25T15:20:52.952449Z","iopub.status.idle":"2025-02-25T15:20:53.058067Z","shell.execute_reply.started":"2025-02-25T15:20:52.952417Z","shell.execute_reply":"2025-02-25T15:20:53.057351Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Loop","metadata":{"papermill":{"duration":0.037279,"end_time":"2025-02-19T20:15:14.450346","exception":false,"start_time":"2025-02-19T20:15:14.413067","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#convertir funciones\nfrom tf_agents.utils.common import function\n\ncollect_driver.run = function(collect_driver.run)\nagent.train = function(agent.train)","metadata":{"papermill":{"duration":0.043798,"end_time":"2025-02-19T20:15:14.529987","exception":false,"start_time":"2025-02-19T20:15:14.486189","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:20:55.880174Z","iopub.execute_input":"2025-02-25T15:20:55.880461Z","iopub.status.idle":"2025-02-25T15:20:55.885377Z","shell.execute_reply.started":"2025-02-25T15:20:55.880437Z","shell.execute_reply":"2025-02-25T15:20:55.884284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_agent(n_iterations):\n    time_step = None\n    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n    iterator = iter(dataset)\n    for iteration in range(n_iterations):\n        time_step, policy_state = collect_driver.run(time_step, policy_state)\n        trajectories, buffer_info = next(iterator)\n        train_loss = agent.train(trajectories)\n        print(\"\\r{} loss: {:.5f}\".format(\n            iteration, train_loss.loss.numpy()), end=\"\")\n        if iteration % 1000 == 0:\n            log_metrics(train_metrics)","metadata":{"papermill":{"duration":0.048056,"end_time":"2025-02-19T20:15:14.616563","exception":false,"start_time":"2025-02-19T20:15:14.568507","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:21:06.547431Z","iopub.execute_input":"2025-02-25T15:21:06.547749Z","iopub.status.idle":"2025-02-25T15:21:06.552542Z","shell.execute_reply.started":"2025-02-25T15:21:06.547721Z","shell.execute_reply":"2025-02-25T15:21:06.551802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#el valor ideal para n_iterations es 1.000.000\nnum_iterations_ = 1000000 #ojo entrenamiento\ntrain_agent(n_iterations=num_iterations_)","metadata":{"papermill":{"duration":613.263022,"end_time":"2025-02-19T20:25:27.91932","exception":false,"start_time":"2025-02-19T20:15:14.656298","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:21:13.276425Z","iopub.execute_input":"2025-02-25T15:21:13.276743Z","iopub.status.idle":"2025-02-25T15:21:18.245321Z","shell.execute_reply.started":"2025-02-25T15:21:13.276718Z","shell.execute_reply":"2025-02-25T15:21:18.244432Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualization","metadata":{"papermill":{"duration":0.135747,"end_time":"2025-02-19T20:25:28.197539","exception":false,"start_time":"2025-02-19T20:25:28.061792","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def update_scene(num, frames, patch):\n    patch.set_data(frames[num])\n    return patch\n\ndef plot_animation(frames, repeat=False, interval=40):\n    fig = plt.figure()\n    patch = plt.imshow(frames[0])\n    plt.axis('off')\n    anim = animation.FuncAnimation(\n        fig, update_scene, fargs=(frames, patch),\n        frames=len(frames), repeat=repeat, interval=interval)\n    plt.close()\n    return anim","metadata":{"papermill":{"duration":0.149535,"end_time":"2025-02-19T20:25:28.485681","exception":false,"start_time":"2025-02-19T20:25:28.336146","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:24:12.972562Z","iopub.execute_input":"2025-02-25T15:24:12.972968Z","iopub.status.idle":"2025-02-25T15:24:12.981114Z","shell.execute_reply.started":"2025-02-25T15:24:12.972923Z","shell.execute_reply":"2025-02-25T15:24:12.980123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"frames = []\ndef save_frames(trajectory):\n    global frames\n    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n\nprev_lives = tf_env.pyenv.envs[0].ale.lives()\ndef reset_and_fire_on_life_lost(trajectory):\n    global prev_lives\n    lives = tf_env.pyenv.envs[0].ale.lives()\n    if prev_lives != lives:\n        tf_env.reset()\n        tf_env.pyenv.envs[0].step(np.array(tf_env.pyenv.envs[0].action_space.sample()))\n        prev_lives = lives\n\nwatch_driver = DynamicStepDriver(\n    tf_env,\n    agent.policy,\n    observers=[save_frames, reset_and_fire_on_life_lost, ShowProgress(10000)],\n    num_steps=10000)\nfinal_time_step, final_policy_state = watch_driver.run()\n\nplot_animation(frames)","metadata":{"papermill":{"duration":266.189637,"end_time":"2025-02-19T20:29:54.813552","exception":false,"start_time":"2025-02-19T20:25:28.623915","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:25:20.796742Z","iopub.execute_input":"2025-02-25T15:25:20.797075Z","iopub.status.idle":"2025-02-25T15:25:21.292262Z","shell.execute_reply.started":"2025-02-25T15:25:20.797049Z","shell.execute_reply":"2025-02-25T15:25:21.29136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Creando un gif\nimport PIL\nimport os\n\nimage_path = os.path.join(\"view1.gif\")\nframe_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]\nframe_images[0].save(image_path, format='GIF',\n                     append_images=frame_images[1:],\n                     save_all=True,\n                     duration=300,\n                     loop=0)","metadata":{"papermill":{"duration":1.034978,"end_time":"2025-02-19T20:29:56.502385","exception":false,"start_time":"2025-02-19T20:29:55.467407","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:25:28.207989Z","iopub.execute_input":"2025-02-25T15:25:28.208292Z","iopub.status.idle":"2025-02-25T15:25:28.251969Z","shell.execute_reply.started":"2025-02-25T15:25:28.208271Z","shell.execute_reply":"2025-02-25T15:25:28.251291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%html\n<img src=\"view9e5.gif\" />","metadata":{"papermill":{"duration":0.655321,"end_time":"2025-02-19T20:29:57.802266","exception":false,"start_time":"2025-02-19T20:29:57.146945","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:25:30.78726Z","iopub.execute_input":"2025-02-25T15:25:30.787564Z","iopub.status.idle":"2025-02-25T15:25:30.793421Z","shell.execute_reply.started":"2025-02-25T15:25:30.787541Z","shell.execute_reply":"2025-02-25T15:25:30.792502Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# References\n[Hands–On Machine Learning with Scikit–Learn and TensorFlow 2](https://www.amazon.com.br/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646)\n\n\n[Agents is a library for reinforcement learning in TensorFlow.\n](https://www.tensorflow.org/agents)\n\n\n[Introduction to TF-Agents : A library for Reinforcement Learning in TensorFlow](https://towardsdatascience.com/introduction-to-tf-agents-a-library-for-reinforcement-learning-in-tensorflow-68ab9add6ad6)\n\n\n[Train a Deep Q Network with TF-Agents](https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial)","metadata":{"papermill":{"duration":0.690125,"end_time":"2025-02-19T20:29:59.135295","exception":false,"start_time":"2025-02-19T20:29:58.44517","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.643395,"end_time":"2025-02-19T20:30:00.408727","exception":false,"start_time":"2025-02-19T20:29:59.765332","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}